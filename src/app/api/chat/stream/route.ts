import { NextResponse } from 'next/server';
import { getKindeServerSession } from "@kinde-oss/kinde-auth-nextjs/server";
import prisma from '@/lib/clients/prisma';
import { getOpenAIClient } from '@/lib/clients/openai';
import { ChatCompletionMessageParam } from 'openai/resources/chat/completions';

export const runtime = 'nodejs';
export const dynamic = 'force-dynamic';

// Handles POST requests to generate and stream an AI response for a chat message
export async function POST(request: Request) {
  try {
    const { chatRoomId, message } = await request.json();

    const { getUser } = getKindeServerSession();
    const user = await getUser();
    
    if (!user?.id) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
    }

    // Get chat room with AI model
    const chatRoom = await prisma.chatRoom.findFirst({
      where: {
        id: chatRoomId,
        users: {
          some: {
            id: user.id
          }
        }
      },
      include: {
        aiModel: true
      }
    });

    if (!chatRoom || !chatRoom.aiModel) {
      return NextResponse.json({ error: 'Chat room not found or AI model missing' }, { status: 404 });
    }

    // Create prompt for AI model
    const systemMessage: ChatCompletionMessageParam = {
      role: "system",
      content: `You are ${chatRoom.aiModel.name}, an AI character with the following traits:
      Personality: ${chatRoom.aiModel.personality}
      Appearance: ${chatRoom.aiModel.appearance}
      Backstory: ${chatRoom.aiModel.backstory}
      Hobbies: ${chatRoom.aiModel.hobbies}
      Likes: ${chatRoom.aiModel.likes}
      Dislikes: ${chatRoom.aiModel.dislikes}
      
      Please respond in character, maintaining these traits consistently.`
    };

    // Get message history
    const messageHistory = await prisma.message.findMany({
      where: {
        chatRoomId,
        createdAt: {
          gte: new Date(Date.now() - 1000 * 60 * 60) // Last hour
        }
      },
      orderBy: {
        createdAt: 'asc'
      },
      take: 10
    });

    // Transform messages for OpenAI format
    const formattedHistory: ChatCompletionMessageParam[] = messageHistory.map(msg => ({
      role: msg.isAIMessage ? "assistant" : "user",
      content: msg.content
    }));

    // Get OpenAI client
    const openAIClient = getOpenAIClient();

    // Create stream
    const stream = await openAIClient?.chat.completions.create({
      model: "gpt-4",
      messages: [
        systemMessage,
        ...formattedHistory,
        { role: "user", content: message }
      ],
      stream: true,
      temperature: 0.9,
      max_tokens: 100,
    });

    // Set up a readable stream to return the AI response in chunks
    const encoder = new TextEncoder();
    const readable = new ReadableStream({
      async start(controller) {
        for await (const chunk of stream!) {
          const content = chunk.choices[0]?.delta?.content || "";
          controller.enqueue(encoder.encode(content));
        }
        controller.close();
      },
    });

    // Return the response stream with appropriate headers for streaming
    return new NextResponse(readable, {
      headers: {
        'Content-Type': 'text/plain',
        'Transfer-Encoding': 'chunked',
      },
    });

  } catch (error) {
    console.error('Error in streaming AI response:', error);
    return NextResponse.json({ error: 'Failed to generate AI response' }, { status: 500 });
  }
}

// This code defines an API endpoint for generating and streaming an AI response for a chat room in a real-time conversational system.
// Here is a step-by-step explanation of what the code does:
// 1. The `POST` function handles incoming POST requests.
// 2. It extracts the `chatRoomId` and `message` from the request body to determine which chat room the message belongs to and the content of the user's message.
// 3. The `getCurrentUser()` function is called to fetch the current logged-in user.
//    - If the user is not authenticated, it returns a 401 Unauthorized response.
// 4. It then fetches the chat room details using the `chatRoomId`, including the users and the associated AI model.
//    - If the chat room is not found or the AI model is missing, it returns a 404 Not Found response.
// 5. A prompt is created for the AI model based on the AI's characteristics and the user's message.
//    - The prompt includes details like personality, appearance, backstory, hobbies, likes, and dislikes of the AI character.
// 6. The OpenAI API is called to generate a response using the GPT-4 model.
//    - The response is streamed back to the client, allowing real-time updates.
// 7. A `ReadableStream` is created to read and enqueue the response chunks generated by OpenAI.
// 8. The API then returns a stream response with appropriate headers (`Content-Type` set to `text/plain` and `Transfer-Encoding` set to `chunked`) to allow for a real-time conversation experience.
// 9. If an error occurs at any point in the process, the error is logged, and a 500 Internal Server Error response is returned.
